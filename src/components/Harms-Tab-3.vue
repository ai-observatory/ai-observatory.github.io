<template>
  <b-tab title="ADMS + Dispossession">
    <p>
      ADMS use in India is facilitating the dispossession of people&rsquo;s
      rights and entitlements without providing adequate recourse to rectify
      unjust dispossession. This section explains why, and how, ADMS are being
      used to determine people&rsquo;s legal entitlements in India, and how it
      is facilitating the dispossession of their legitimate claims to state
      welfare and access to public goods.
    </p>
    <p>
      Algorithmic systems are widely used by public agencies in India, in order
      to <em>identify or screen </em>individuals who may receive government
      welfare or access to rights; to determine their<em> eligibility </em>for
      particular entitlements; to <em>sort and classify</em> eligible
      beneficiaries<em>, </em>as well as to determine the circumstances which
      can render them ineligible for these entitlements. Each of these uses of
      ADMS has led to barriers in access to welfare or the exercise of
      individual rights, and the dispossession of peoples claims and
      entitlements to varying degrees.
    </p>
    <p><strong>Identification</strong></p>
    <p>
      Welfare distribution in India currently depends extensively on the
      identification of particular individuals and groups entitled to specific
      state benefits like ration, housing or credit. Algorithmic systems are
      being used to identify whether welfare beneficiaries and rights-holders
      are who they claim to be. There has been a systematic effort to digitise
      identification systems and utilise ADMS for the purpose of identification,
      such as in the move from paper documents for identification to the use of
      digital biometric systems in the case of Aadhaar.
    </p>
    <p>
      As explained previously, the Government of India&rsquo;s Aadhaar system
      utilises biometric fingerprint recognition technologies to ensure that
      beneficiaries are correctly identified. This relies on the faulty
      assumption that biometric identification is accurate across populations,
      even as the Government has itself claimed that the biometric
      authentication mechanism fails at multiple levels. The Government of India
      is now attempting to utilise<a
        href="https://economictimes.indiatimes.com/industry/banking/finance/banking/facial-recognition-iris-scans-may-be-used-for-welfare-scheme-payouts/articleshow/77755102.cms?from=mdr"
      >
        iris scanning and facial recognition</a
      >
      technologies for biometric authentication within the Aadhaar system, each
      of which come with their own biases and points of failure.
    </p>
    <p><strong>Eligibility, Screening and Prioritisation </strong></p>
    <p>
      Welfare systems across the world, including in India, depend extensively
      on filtering and classifying individuals in order to ensure that benefits
      are claimed according to particular socio-economic circumstances. For
      example, the eligibility of individuals to central and state government
      welfare schemes like PDS or is often determined against the enumeration of
      individuals and households done through the
      <a
        href="https://economictimes.indiatimes.com/news/politics-and-nation/government-to-use-secc-data-for-effectiveness-of-welfare-schemes/articleshow/57001274.cms?utm_source=contentofinterest&amp;utm_medium=text&amp;utm_campaign=cppst"
        >Socio-Economic and Caste Census</a
      >
      (last conducted in 2011), or through household surveys conducted at the
      state level.
    </p>
    <p>
      Increasingly, however, classifications created by algorithmic systems are
      being used as alternatives to, or in addition to, criteria which have
      historically determined eligibility for access to welfare or legal
      entitlements. This is done both to determine their entry into the welfare
      system (whether they are considered &lsquo;eligible beneficiaries&rsquo;),
      as well as to classify and sort beneficiaries among themselves, according
      to some criterion for priority.
    </p>
    <p>
      For example, the Government of Telangana&rsquo;s &lsquo;Samagra
      Vedika&rsquo; scheme is being used to create additional classifications to
      determine whether an individual or a family qualifies for &lsquo;Below
      Poverty Line&rsquo; schemes like the Government&rsquo;s subsidised housing
      scheme, by analysing data across a number of fields, including vehicle
      registration, electricity bills or property taxes. According to publicly
      available information, an algorithmic system is being utilised to assess
      whether an individual should be granted benefits based on an analysis of
      information located in various databases, without individuals being given
      the opportunity of a hearing or being informed of how such information is
      to be used and analysed by the system. The system classifies beneficiaries
      according to four classes &ndash; &lsquo;qualify&rsquo;, &lsquo;qualify
      with verification&rsquo;, &lsquo;low priority&rsquo; and
      &lsquo;don&rsquo;t consider&rsquo;. The benefits are then dispersed
      according to the classification generated, with different consequences for
      different classes of individuals (for example, &lsquo;don&rsquo;t
      consider&rsquo; individuals will not be provided claims, while &lsquo;low
      priority&rsquo; will only be considered after disbursal to the
      &lsquo;qualified&rsquo; candidates).
    </p>

    <div>
      <b-img
        center
        style="width: 100%; height: 650px; object-fit: cover"
        src="../../public/icon2.png"
        fluid
        alt="surveillance and profiling harms"
      ></b-img>
    </div>

    <p>
      The outputs of algorithmic systems are also being used to
      &lsquo;screen&rsquo; individuals and determine their access to public
      spaces. For example, many airports in India have started using facial
      recognition screening to augment and replace human checks for boarding
      passes. Similarly, automated systems are determining access to public
      spaces by screening attendees &ndash; including political rallies. In New
      Delhi, the Delhi Police&rsquo;s Automated Facial Recognition System was
      used to screen persons who were identified as &lsquo;habitual
      protestors&rsquo;, and
      <a
        href="https://thewire.in/government/delhi-police-is-now-using-facial-recognition-software-to-screen-habitual-protestors"
        >disallow</a
      >
      them from attending a political rally organised by Prime Minister Narendra
      Modi.
    </p>

    <div class="modal-button-container">
      <b-button class="modal-button" v-b-modal="'case-study-5'"
        >[Case Study: Algorithmic Health Screening Through Aarogya
        Setu]</b-button
      >
      <b-modal id="case-study-5" size="xl" hide-footer>
        <div class="d-block text-left">
          <h3>Case Study: Algorithmic Health Screening Through Aarogya Setu</h3>
          <p>
            Automated screening tools have been widely utilised to prevent
            potential outbreaks or contagion during the CoVID-19 pandemic. One
            such tool has been Aarogya Setu, an ADMS deployed by the Government
            of India, which is expected to track a person&rsquo;s movement and
            their associations with other people, in order to model their risk
            of infection of COVID-19.<sup
              ><a href="#sdfootnote1sym" name="sdfootnote1anc">1</a></sup
            >
          </p>
          <div>
            <b-img
              center
              style="width: 100%; height: 650px; object-fit: cover"
              src="../../public/icon3.png"
              fluid
              alt="aarogya setu"
            ></b-img>
          </div>
          <p>
            Aarogya Setu embodied the failure of legal and government
            institutions to recognise and account for the failure of ADMS. While
            the tool was initially used only as a voluntary and private
            mechanism for individuals to rely upon, the purpose of the tool soon
            changed. Government notifications made the use of Aarogya Setu
            mandatory for accessing public spaces and services, including for
            employees to access their workspaces, as well as for the use of
            public transport &ndash; from flights to trains.
          </p>
          <p>
            The tool quickly transformed into an automated screening system
            &ndash; the data collected from the system was used to classify
            individual users based on risk of infection, and to assign
            particular values (high risk, medium risk and low risk) to users.
            Access to public spaces was then mediated based on the values
            assigned to individual users &ndash; without individuals being made
            aware of the specific reason for the value, nor any mechanism to
            challenge or appeal the decision made by the tool.
          </p>
          <p>
            The example of Aarogya Setu should caution us not only to the
            inherent limitations of certain technological systems, but also,
            importantly, of how reliance on automated digital technologies can
            exclude populations who do not have access to them and who cannot be
            &lsquo;seen&rsquo; by the data alone.
          </p>
        </div>
      </b-modal>
    </div>

    <p><strong>Fraud and Duplicate Detection</strong></p>
    <p>
      A related and common use of the system is to render ineligible and stop
      the benefits or entitlements of individuals who have been flagged by an
      automated system as either &lsquo;duplicates&rsquo; or
      &lsquo;fraudulent&rsquo;. As explained previously, de-duplication of
      databases in order to ensure the &lsquo;uniqueness&rsquo; of beneficiaries
      has been a major fixation of contemporary ADMS use in public agencies.
      These de-duplication algorithms are being utilised across welfare schemes
      and are leading to the generation of &lsquo;clean&rsquo; lists which often
      deprive and dispossess individuals of their legal claims, from welfare
      entitlements like ration or credit, to voting rights.
    </p>

    <div class="modal-button-container">
      <b-button class="modal-button" v-b-modal="'case-study-6'"
        >[Case Study: Disenfranchising Voters Through Algorithmic
        Purification]</b-button
      >
      <b-modal id="case-study-6" size="xl" hide-footer>
        <div class="d-block text-left">
          <h3>
            Case Study: Disenfranchising Voters Through Algorithmic Purification
          </h3>
          <p>
            The National Electoral Roll Purification and Authentication
            Programme, or NERPAP, is a project of the Election Commission of
            India intended to &lsquo;de-duplicate&rsquo; the electoral rolls for
            registered voters, in an effort to curtail election fraud. Initially
            intended to be voluntary, the NERPAP programme has subsequently been
            expanded and &lsquo;piloted&rsquo; in various jurisdictions without
            the consent of voters, leading to wide-scale deletion of eligible
            names from voter lists &ndash; and consequently, disenfranchising
            voters from exercising their right to vote. From Bihar to Telangana
            &ndash; the implementation of the NERPAP ADMS has resulted in the
            wide-scale disenfranchisement.
          </p>

          <div>
            <b-img
              center
              style="width: 100%; height: 650px; object-fit: cover"
              src="../../public/icon5.png"
              fluid
              alt="voters"
            ></b-img>
          </div>

          <p>
            The NERPAP programme identified potential duplicate or
            &lsquo;fraudulent&rsquo; voters through the process of matching
            names in voter databases (the Electoral Photo ID Card or EPIC
            database), to citizen databases which were linked with an Aadhaar
            ID. If the algorithmic system which matched the two databases saw
            &lsquo;duplicates&rsquo;, these names were automatically deleted
            from the list of voters. The curious process followed in the NERPAP
            programme assumed fraud if indicated by the name-deletion algorithm,
            and placed the burden of responding to the deletion on citizens,
            after the fact. While the algorithmic &lsquo;seeding&rsquo; of
            Aadhaar with NERPAP was finally halted through the
            <a
              href="https://rti.eci.nic.in/public/images/cpio_uploaded/3769/13.08.2015(NERPAP-ban%20on%20Aadhaar%20linking%20with%20ER%20database).pdf"
              >intervention</a
            >
            of the Supreme Court of India, the lasting damage to the exercise of
            a right to democratic participation has neither been acknowledged
            nor mitigated by the agencies responsible for the system. Rather,
            according to reports, the Government of India is intending to give
            legal sanctity to the system through amendments to the law.
          </p>
        </div>
      </b-modal>
    </div>

    <p>
      Apart from efforts at removing &lsquo;digital duplicates&rsquo;,
      progressively sophisticated &lsquo;automated fraud detection
      systems&rsquo; abound in public agencies &ndash; from the Income Tax
      department to public health insurance through the Ayushman Bharat scheme.
      Increasingly, these agencies are adopting machine learning systems for
      identifying fraudulent behaviour, including systems analyse information
      across a range of databases, from government databases to social media.
    </p>

    <div class="modal-button-container">
      <b-button class="modal-button" v-b-modal="'case-study-7'"
        >[Case Study: Automated Fraud Detection in the Ayushman Bharat Universal
        Health Scheme]</b-button
      >
      <b-modal id="case-study-7" size="xl" hide-footer>
        <div class="d-block text-left">
          <h3>
            [Case Study: Automated Fraud Detection in the Ayushman Bharat
            Universal Health Scheme]
          </h3>
          <p>
            Ayushman Bharat is the Government of India&rsquo;s ambitious
            universal healthcare scheme, which intends to provide health
            insurance coverage to 40% of the lowest-income population in India.
            Fraudulent claims towards health insurance have been described as a
            major obstacle in the implementation of the scheme. To overcome this
            obstacle, the National Health Authority, responsible for the
            administration of Ayushman Bharat, has implemented a &lsquo;Fraud
            Analytics Control and Tracking System&rsquo;, developed by the firm
            SAS.
          </p>

          <div>
            <b-img
              center
              style="width: 100%; height: 650px; object-fit: cover"
              src="../../public/icon4.png"
              fluid
              alt=""
            ></b-img>
          </div>

          <p>
            According to the
            <a
              href="https://pmjay.gov.in/sites/default/files/2019-09/Annual%20Report%20-%20PMJAY%20small%20version_1.pdf"
              >NHA</a
            >, the FACTS system will use Artificial Intelligence and Machine
            Learning in order to
            <em
              >&ldquo;identify suspect transactions &amp; entities. Using
              advanced tools such as Natural Language Processing and Optical
              Character Recognition and Image Analytics, unstructured data such
              as images, documents and clinical notes submitted are analysed to
              detect cases of potential fraud and abuse.&rdquo; </em
            >As per the Anti-Fraud Guidelines issued by the National Health
            Authority, the fraud identification software will be retrospective
            &ndash; to assess patterns of fraud from historical claims, as well
            as assessing claims on a case-by-case basis. Therefore, the software
            is possibly being leveraged to accept or deny insurance claims under
            the scheme. It is as yet unclear what particular algorithmic models
            or datasets will be utilised in the fraud analytics software.
          </p>
          <p>
            What are the consequences of the software flagging a transaction or
            an individual as &lsquo;fraudulent&rsquo;? According to the
            Anti-Fraud Guidelines, the Anti-Fraud Cell to whom the fraud has
            been notified is supposed to ascertain whether there is
            <em>prima facie </em>evidence of fraud, and if this is found, then
            to conduct a full investigation which can result in the rejection of
            an insurance claim and disciplinary action.
          </p>
          <p>
            Systems like FACTS appear to be effective methods of achieving a
            legitimate aim &ndash; of curtailing fraudulent behaviour. However,
            in the absence of transparent processes and accountability measures
            where wrong decisions can be challenged, these systems can result in
            individuals becoming embroiled in invasive surveillance and
            pecuniary processes, at times when they are most vulnerable &ndash;
            such as during a medical emergency. These systems can compromise
            important democratic values and human rights &ndash; including the
            right to privacy and the right to challenge an adverse decision.
            Recognising this, a similar fraud analytics system, known as the
            System Risk Indicator or SyRI, was struck down on grounds of
            violating the European Convention on Human Rights by a Dutch Court
            in the Hague.<sup
              ><a href="#sdfootnote2sym" name="sdfootnote2anc"
                ><sup>2</sup></a
              ></sup
            >
          </p>
        </div>
      </b-modal>
    </div>

    <p>
      The use of ADMS to determine the scope and extent of rights and
      entitlements can lead to massive disentitlement and dispossession, without
      providing adequate justification, and in ways which can be difficult to
      uncover or report. With government agencies wholeheartedly endorsing
      &lsquo;data-based decision making&rsquo; for welfare, we must remain
      cautious of the ways in which these systems can cause injustice at scale,
      particularly to populations who are the most dependent on the state for
      their social security and safety.
    </p>

    <div class="footnotes">
      <div>
      <p>
        <a href="#sdfootnote1anc" name="sdfootnote1sym">1</a> Joshi, D., Mohan,
        S., &lsquo;A Legal Framework for Digital Surveillance in the COVID-19
        Pandemic&rsquo;, Medianama, (July 14, 2020)
      </p>
    </div>
    <div>
      <p>
        <a href="#sdfootnote2anc" name="sdfootnote2sym">2</a> Meuwese, A.
        (2020). Regulating algorithmic decision-making one case at the time.
        Case note on: District Courtof The Hague , 5/02/20,
        ECLI:NL:RBDHA:2020:865 (NJCM vs the Netherlands (SyRI)). European Review
        of Digital Administration &amp; Law, 1(1), 209-211.
      </p>
    </div>
    </div>
  </b-tab>
</template>

<script></script>

<style lang="scss" scoped></style>
